-all agents have complete observability; they can see every other agent
-non stationarity is a key issue in decentralised MARL
-implement a proximal policy optimisation (PPO). This is like deep-q learning but only stores a
 small batch of experiences.
 -there exist a few types of RL; policy gradient, 
 -continuous state/observation space, but discrete action space is needed.

 -implementation decision:
    -policy/q-tables:   //Use distributed, indiviudal policy networks/q-tables for each individual
        -one shared policy/q-table//
        -individual policy/q-tables for each individual//
        -multiple populations, where individuals in a population sharea policy/q-table//
    -state representations for the policy/q-tables:
        -how many it is stronger/weaker than for first dim, and stronger/weaker for second dim
        -the reward dictionary
        -the agent dictionary  
    -agent actions:
        -agents can decrease their values
        -agents can change two values at a time
        -agents can increase/decrease values by any amount
